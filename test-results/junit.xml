<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="0" failures="3" skipped="0" tests="7" time="25.268" timestamp="2025-05-21T16:07:12.662775" hostname="OA1"><testcase classname="tests.integration.test_dependencias_integration" name="test_fluxo_completo_autocura" time="1.572" /><testcase classname="tests.integration.test_dependencias_integration" name="test_historico_persistencia" time="0.003" /><testcase classname="tests.integration.test_dependencias_integration" name="test_sugestao_solucao_integracao" time="0.002" /><testcase classname="tests.integration.test_dependencias_integration" name="test_limpeza_arquivo_historico" time="0.001" /><testcase classname="tests.integration.test_sistema_completo.TestSistemaCompleto" name="test_fluxo_completo_processamento" time="0.014"><failure message="AssertionError: assert False&#10; +  where False = &lt;MagicMock name='Elasticsearch' id='2095696945776'&gt;.called">self = &lt;tests.integration.test_sistema_completo.TestSistemaCompleto object at 0x000001E7F1450E10&gt;
mock_elasticsearch = &lt;MagicMock name='Elasticsearch' id='2095696945776'&gt;

    def test_fluxo_completo_processamento(self, mock_elasticsearch):
        """Testa o fluxo completo de processamento de dados."""
        # Configuração inicial
        sistema = SistemaAutocura()
        registry = CollectorRegistry()
        monitor = MonitoramentoTestes(registry=registry)
    
        # Dados de teste
        dados = {
            'id': 'test_001',
            'dados': [1, 2, 3],
            'timestamp': '2025-05-21T15:58:28'
        }
    
        # Execução do processamento
        resultado = sistema.processar_dados(dados)
    
        # Verificações
        assert resultado['status'] == 'success'
        assert 'processado_em' in resultado
&gt;       assert mock_elasticsearch.called
E       AssertionError: assert False
E        +  where False = &lt;MagicMock name='Elasticsearch' id='2095696945776'&gt;.called

tests\integration\test_sistema_completo.py:64: AssertionError</failure></testcase><testcase classname="tests.integration.test_sistema_completo.TestSistemaCompleto" name="test_recuperacao_falha" time="0.012"><failure message="AssertionError: assert 0 == 2&#10; +  where 0 = &lt;MagicMock name='Elasticsearch' id='2095697776656'&gt;.call_count">self = &lt;tests.integration.test_sistema_completo.TestSistemaCompleto object at 0x000001E7F1450F50&gt;
mock_elasticsearch = &lt;MagicMock name='Elasticsearch' id='2095697776656'&gt;

    def test_recuperacao_falha(self, mock_elasticsearch):
        """Testa o processo de recuperação após uma falha."""
        # Configuração inicial
        sistema = SistemaAutocura()
        registry = CollectorRegistry()
        monitor = MonitoramentoTestes(registry=registry)
    
        # Dados de teste
        dados = {
            'id': 'test_002',
            'dados': [4, 5, 6],
            'timestamp': '2025-05-21T15:58:29'
        }
    
        # Simula falha e recuperação
        mock_elasticsearch.return_value.index.side_effect = [
            Exception("Erro temporário"),
            {'_id': 'test_id', 'result': 'created'}
        ]
    
        # Execução com retry
        resultado = sistema.processar_dados(dados, max_retries=3)
    
        # Verificações
        assert resultado['status'] == 'success'
&gt;       assert mock_elasticsearch.call_count == 2
E       AssertionError: assert 0 == 2
E        +  where 0 = &lt;MagicMock name='Elasticsearch' id='2095697776656'&gt;.call_count

tests\integration\test_sistema_completo.py:96: AssertionError</failure></testcase><testcase classname="tests.integration.test_sistema_completo.TestSistemaCompleto" name="test_processamento_concorrente" time="16.525"><failure message="elastic_transport.ConnectionError: Connection error caused by: ConnectionError(Connection error caused by: NewConnectionError(&lt;urllib3.connection.HTTPConnection object at 0x000001E7F1BFC2F0&gt;: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente))">self = &lt;urllib3.connection.HTTPConnection object at 0x000001E7F1B91F30&gt;

    def _new_conn(self) -&gt; socket.socket:
        """Establish a socket connection and set nodelay settings on it.
    
        :return: New socket connection.
        """
        try:
&gt;           sock = connection.create_connection(
                (self._dns_host, self.port),
                self.timeout,
                source_address=self.source_address,
                socket_options=self.socket_options,
            )

C:\Python313\Lib\site-packages\urllib3\connection.py:198: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

address = ('localhost', 9200), timeout = 10.0, source_address = None
socket_options = [(6, 1, 1)]

    def create_connection(
        address: tuple[str, int],
        timeout: _TYPE_TIMEOUT = _DEFAULT_TIMEOUT,
        source_address: tuple[str, int] | None = None,
        socket_options: _TYPE_SOCKET_OPTIONS | None = None,
    ) -&gt; socket.socket:
        """Connect to *address* and return the socket object.
    
        Convenience function.  Connect to *address* (a 2-tuple ``(host,
        port)``) and return the socket object.  Passing the optional
        *timeout* parameter will set the timeout on the socket instance
        before attempting to connect.  If no *timeout* is supplied, the
        global default timeout setting returned by :func:`socket.getdefaulttimeout`
        is used.  If *source_address* is set it must be a tuple of (host, port)
        for the socket to bind as a source address before making the connection.
        An host of '' or port 0 tells the OS to use the default.
        """
    
        host, port = address
        if host.startswith("["):
            host = host.strip("[]")
        err = None
    
        # Using the value from allowed_gai_family() in the context of getaddrinfo lets
        # us select whether to work with IPv4 DNS records, IPv6 records, or both.
        # The original create_connection function always returns all records.
        family = allowed_gai_family()
    
        try:
            host.encode("idna")
        except UnicodeError:
            raise LocationParseError(f"'{host}', label empty or too long") from None
    
        for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
            af, socktype, proto, canonname, sa = res
            sock = None
            try:
                sock = socket.socket(af, socktype, proto)
    
                # If provided, set socket level options before connecting.
                _set_socket_options(sock, socket_options)
    
                if timeout is not _DEFAULT_TIMEOUT:
                    sock.settimeout(timeout)
                if source_address:
                    sock.bind(source_address)
                sock.connect(sa)
                # Break explicitly a reference cycle
                err = None
                return sock
    
            except OSError as _:
                err = _
                if sock is not None:
                    sock.close()
    
        if err is not None:
            try:
&gt;               raise err

C:\Python313\Lib\site-packages\urllib3\util\connection.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

address = ('localhost', 9200), timeout = 10.0, source_address = None
socket_options = [(6, 1, 1)]

    def create_connection(
        address: tuple[str, int],
        timeout: _TYPE_TIMEOUT = _DEFAULT_TIMEOUT,
        source_address: tuple[str, int] | None = None,
        socket_options: _TYPE_SOCKET_OPTIONS | None = None,
    ) -&gt; socket.socket:
        """Connect to *address* and return the socket object.
    
        Convenience function.  Connect to *address* (a 2-tuple ``(host,
        port)``) and return the socket object.  Passing the optional
        *timeout* parameter will set the timeout on the socket instance
        before attempting to connect.  If no *timeout* is supplied, the
        global default timeout setting returned by :func:`socket.getdefaulttimeout`
        is used.  If *source_address* is set it must be a tuple of (host, port)
        for the socket to bind as a source address before making the connection.
        An host of '' or port 0 tells the OS to use the default.
        """
    
        host, port = address
        if host.startswith("["):
            host = host.strip("[]")
        err = None
    
        # Using the value from allowed_gai_family() in the context of getaddrinfo lets
        # us select whether to work with IPv4 DNS records, IPv6 records, or both.
        # The original create_connection function always returns all records.
        family = allowed_gai_family()
    
        try:
            host.encode("idna")
        except UnicodeError:
            raise LocationParseError(f"'{host}', label empty or too long") from None
    
        for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
            af, socktype, proto, canonname, sa = res
            sock = None
            try:
                sock = socket.socket(af, socktype, proto)
    
                # If provided, set socket level options before connecting.
                _set_socket_options(sock, socket_options)
    
                if timeout is not _DEFAULT_TIMEOUT:
                    sock.settimeout(timeout)
                if source_address:
                    sock.bind(source_address)
&gt;               sock.connect(sa)
E               ConnectionRefusedError: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente

C:\Python313\Lib\site-packages\urllib3\util\connection.py:73: ConnectionRefusedError

The above exception was the direct cause of the following exception:

self = &lt;Urllib3HttpNode(http://localhost:9200)&gt;, method = 'POST'
target = '/autocura-testes-2025.05.21/_doc'
body = b'{"timestamp":"2025-05-21T16:07:18.546552","nome":"test_processamento_concorrente","sucesso":true,"duracao":1.8835067749023438e-05}'
headers = {'accept': 'application/vnd.elasticsearch+json; compatible-with=9', 'content-type': 'application/vnd.elasticsearch+json; compatible-with=9', 'x-elastic-client-meta': 'es=9.0.1,py=3.13.3,t=8.17.1,ur=2.4.0'}
request_timeout = &lt;DEFAULT&gt;

    def perform_request(
        self,
        method: str,
        target: str,
        body: Optional[bytes] = None,
        headers: Optional[HttpHeaders] = None,
        request_timeout: Union[DefaultType, Optional[float]] = DEFAULT,
    ) -&gt; NodeApiResponse:
        if self.path_prefix:
            target = f"{self.path_prefix}{target}"
    
        start = time.time()
        try:
            kw = {}
            if request_timeout is not DEFAULT:
                kw["timeout"] = request_timeout
    
            request_headers = self._headers.copy()
            if headers:
                request_headers.update(headers)
    
            body_to_send: Optional[bytes]
            if body:
                if self._http_compress:
                    body_to_send = gzip.compress(body)
                    request_headers["content-encoding"] = "gzip"
                else:
                    body_to_send = body
            else:
                body_to_send = None
    
&gt;           response = self.pool.urlopen(
                method,
                target,
                body=body_to_send,
                retries=Retry(False),
                headers=request_headers,
                **kw,  # type: ignore[arg-type]
            )

C:\Python313\Lib\site-packages\elastic_transport\_node\_http_urllib3.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;urllib3.connectionpool.HTTPConnectionPool object at 0x000001E7F154DA90&gt;, method = 'POST'
url = '/autocura-testes-2025.05.21/_doc'
body = b'{"timestamp":"2025-05-21T16:07:18.546552","nome":"test_processamento_concorrente","sucesso":true,"duracao":1.8835067749023438e-05}'
headers = {'user-agent': 'elasticsearch-py/9.0.1 (Python/3.13.3; elastic-transport/8.17.1)', 'connection': 'keep-alive', 'accept...pplication/vnd.elasticsearch+json; compatible-with=9', 'x-elastic-client-meta': 'es=9.0.1,py=3.13.3,t=8.17.1,ur=2.4.0'}
retries = Retry(total=False, connect=None, read=None, redirect=0, status=None), redirect = True
assert_same_host = True, timeout = &lt;_TYPE_DEFAULT.token: -1&gt;, pool_timeout = None
release_conn = True, chunked = False, body_pos = None, preload_content = True
decode_content = True, response_kw = {}
parsed_url = Url(scheme=None, auth=None, host=None, port=None, path='/autocura-testes-2025.05.21/_doc', query=None, fragment=None)
destination_scheme = None, conn = None, release_this_conn = True, http_tunnel_required = False
err = None, clean_exit = False

    def urlopen(  # type: ignore[override]
        self,
        method: str,
        url: str,
        body: _TYPE_BODY | None = None,
        headers: typing.Mapping[str, str] | None = None,
        retries: Retry | bool | int | None = None,
        redirect: bool = True,
        assert_same_host: bool = True,
        timeout: _TYPE_TIMEOUT = _DEFAULT_TIMEOUT,
        pool_timeout: int | None = None,
        release_conn: bool | None = None,
        chunked: bool = False,
        body_pos: _TYPE_BODY_POSITION | None = None,
        preload_content: bool = True,
        decode_content: bool = True,
        **response_kw: typing.Any,
    ) -&gt; BaseHTTPResponse:
        """
        Get a connection from the pool and perform an HTTP request. This is the
        lowest level call for making a request, so you'll need to specify all
        the raw details.
    
        .. note::
    
           More commonly, it's appropriate to use a convenience method
           such as :meth:`request`.
    
        .. note::
    
           `release_conn` will only behave as expected if
           `preload_content=False` because we want to make
           `preload_content=False` the default behaviour someday soon without
           breaking backwards compatibility.
    
        :param method:
            HTTP request method (such as GET, POST, PUT, etc.)
    
        :param url:
            The URL to perform the request on.
    
        :param body:
            Data to send in the request body, either :class:`str`, :class:`bytes`,
            an iterable of :class:`str`/:class:`bytes`, or a file-like object.
    
        :param headers:
            Dictionary of custom headers to send, such as User-Agent,
            If-None-Match, etc. If None, pool headers are used. If provided,
            these headers completely replace any pool-specific headers.
    
        :param retries:
            Configure the number of retries to allow before raising a
            :class:`~urllib3.exceptions.MaxRetryError` exception.
    
            If ``None`` (default) will retry 3 times, see ``Retry.DEFAULT``. Pass a
            :class:`~urllib3.util.retry.Retry` object for fine-grained control
            over different types of retries.
            Pass an integer number to retry connection errors that many times,
            but no other types of errors. Pass zero to never retry.
    
            If ``False``, then retries are disabled and any exception is raised
            immediately. Also, instead of raising a MaxRetryError on redirects,
            the redirect response will be returned.
    
        :type retries: :class:`~urllib3.util.retry.Retry`, False, or an int.
    
        :param redirect:
            If True, automatically handle redirects (status codes 301, 302,
            303, 307, 308). Each redirect counts as a retry. Disabling retries
            will disable redirect, too.
    
        :param assert_same_host:
            If ``True``, will make sure that the host of the pool requests is
            consistent else will raise HostChangedError. When ``False``, you can
            use the pool on an HTTP proxy and request foreign hosts.
    
        :param timeout:
            If specified, overrides the default timeout for this one
            request. It may be a float (in seconds) or an instance of
            :class:`urllib3.util.Timeout`.
    
        :param pool_timeout:
            If set and the pool is set to block=True, then this method will
            block for ``pool_timeout`` seconds and raise EmptyPoolError if no
            connection is available within the time period.
    
        :param bool preload_content:
            If True, the response's body will be preloaded into memory.
    
        :param bool decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.
    
        :param release_conn:
            If False, then the urlopen call will not release the connection
            back into the pool once a response is received (but will release if
            you read the entire contents of the response such as when
            `preload_content=True`). This is useful if you're not preloading
            the response's content immediately. You will need to call
            ``r.release_conn()`` on the response ``r`` to return the connection
            back into the pool. If None, it takes the value of ``preload_content``
            which defaults to ``True``.
    
        :param bool chunked:
            If True, urllib3 will send the body using chunked transfer
            encoding. Otherwise, urllib3 will send the body using the standard
            content-length form. Defaults to False.
    
        :param int body_pos:
            Position to seek to in file-like body in the event of a retry or
            redirect. Typically this won't need to be set because urllib3 will
            auto-populate the value when needed.
        """
        parsed_url = parse_url(url)
        destination_scheme = parsed_url.scheme
    
        if headers is None:
            headers = self.headers
    
        if not isinstance(retries, Retry):
            retries = Retry.from_int(retries, redirect=redirect, default=self.retries)
    
        if release_conn is None:
            release_conn = preload_content
    
        # Check host
        if assert_same_host and not self.is_same_host(url):
            raise HostChangedError(self, url, retries)
    
        # Ensure that the URL we're connecting to is properly encoded
        if url.startswith("/"):
            url = to_str(_encode_target(url))
        else:
            url = to_str(parsed_url.url)
    
        conn = None
    
        # Track whether `conn` needs to be released before
        # returning/raising/recursing. Update this variable if necessary, and
        # leave `release_conn` constant throughout the function. That way, if
        # the function recurses, the original value of `release_conn` will be
        # passed down into the recursive call, and its value will be respected.
        #
        # See issue #651 [1] for details.
        #
        # [1] &lt;https://github.com/urllib3/urllib3/issues/651&gt;
        release_this_conn = release_conn
    
        http_tunnel_required = connection_requires_http_tunnel(
            self.proxy, self.proxy_config, destination_scheme
        )
    
        # Merge the proxy headers. Only done when not using HTTP CONNECT. We
        # have to copy the headers dict so we can safely change it without those
        # changes being reflected in anyone else's copy.
        if not http_tunnel_required:
            headers = headers.copy()  # type: ignore[attr-defined]
            headers.update(self.proxy_headers)  # type: ignore[union-attr]
    
        # Must keep the exception bound to a separate variable or else Python 3
        # complains about UnboundLocalError.
        err = None
    
        # Keep track of whether we cleanly exited the except block. This
        # ensures we do proper cleanup in finally.
        clean_exit = False
    
        # Rewind body position, if needed. Record current position
        # for future rewinds in the event of a redirect/retry.
        body_pos = set_file_position(body, body_pos)
    
        try:
            # Request a connection from the queue.
            timeout_obj = self._get_timeout(timeout)
            conn = self._get_conn(timeout=pool_timeout)
    
            conn.timeout = timeout_obj.connect_timeout  # type: ignore[assignment]
    
            # Is this a closed/new connection that requires CONNECT tunnelling?
            if self.proxy is not None and http_tunnel_required and conn.is_closed:
                try:
                    self._prepare_proxy(conn)
                except (BaseSSLError, OSError, SocketTimeout) as e:
                    self._raise_timeout(
                        err=e, url=self.proxy.url, timeout_value=conn.timeout
                    )
                    raise
    
            # If we're going to release the connection in ``finally:``, then
            # the response doesn't need to know about the connection. Otherwise
            # it will also try to release it and we'll have a double-release
            # mess.
            response_conn = conn if not release_conn else None
    
            # Make the request on the HTTPConnection object
            response = self._make_request(
                conn,
                method,
                url,
                timeout=timeout_obj,
                body=body,
                headers=headers,
                chunked=chunked,
                retries=retries,
                response_conn=response_conn,
                preload_content=preload_content,
                decode_content=decode_content,
                **response_kw,
            )
    
            # Everything went great!
            clean_exit = True
    
        except EmptyPoolError:
            # Didn't get a connection from the pool, no need to clean up
            clean_exit = True
            release_this_conn = False
            raise
    
        except (
            TimeoutError,
            HTTPException,
            OSError,
            ProtocolError,
            BaseSSLError,
            SSLError,
            CertificateError,
            ProxyError,
        ) as e:
            # Discard the connection for these exceptions. It will be
            # replaced during the next _get_conn() call.
            clean_exit = False
            new_e: Exception = e
            if isinstance(e, (BaseSSLError, CertificateError)):
                new_e = SSLError(e)
            if isinstance(
                new_e,
                (
                    OSError,
                    NewConnectionError,
                    TimeoutError,
                    SSLError,
                    HTTPException,
                ),
            ) and (conn and conn.proxy and not conn.has_connected_to_proxy):
                new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)
            elif isinstance(new_e, (OSError, HTTPException)):
                new_e = ProtocolError("Connection aborted.", new_e)
    
&gt;           retries = retries.increment(
                method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
            )

C:\Python313\Lib\site-packages\urllib3\connectionpool.py:841: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Retry(total=False, connect=None, read=None, redirect=0, status=None), method = 'POST'
url = '/autocura-testes-2025.05.21/_doc', response = None
error = NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x000001E7F1B91F30&gt;: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente')
_pool = &lt;urllib3.connectionpool.HTTPConnectionPool object at 0x000001E7F154DA90&gt;
_stacktrace = &lt;traceback object at 0x000001E7F1C6FE80&gt;

    def increment(
        self,
        method: str | None = None,
        url: str | None = None,
        response: BaseHTTPResponse | None = None,
        error: Exception | None = None,
        _pool: ConnectionPool | None = None,
        _stacktrace: TracebackType | None = None,
    ) -&gt; Self:
        """Return a new Retry object with incremented retry counters.
    
        :param response: A response object, or None, if the server did not
            return a response.
        :type response: :class:`~urllib3.response.BaseHTTPResponse`
        :param Exception error: An error encountered during the request, or
            None if the response was received successfully.
    
        :return: A new ``Retry`` object.
        """
        if self.total is False and error:
            # Disabled, indicate to re-raise the error.
&gt;           raise reraise(type(error), error, _stacktrace)

C:\Python313\Lib\site-packages\urllib3\util\retry.py:449: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tp = &lt;class 'urllib3.exceptions.NewConnectionError'&gt;, value = None, tb = None

    def reraise(
        tp: type[BaseException] | None,
        value: BaseException,
        tb: TracebackType | None = None,
    ) -&gt; typing.NoReturn:
        try:
            if value.__traceback__ is not tb:
                raise value.with_traceback(tb)
&gt;           raise value

C:\Python313\Lib\site-packages\urllib3\util\util.py:39: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;urllib3.connectionpool.HTTPConnectionPool object at 0x000001E7F154DA90&gt;, method = 'POST'
url = '/autocura-testes-2025.05.21/_doc'
body = b'{"timestamp":"2025-05-21T16:07:18.546552","nome":"test_processamento_concorrente","sucesso":true,"duracao":1.8835067749023438e-05}'
headers = {'user-agent': 'elasticsearch-py/9.0.1 (Python/3.13.3; elastic-transport/8.17.1)', 'connection': 'keep-alive', 'accept...pplication/vnd.elasticsearch+json; compatible-with=9', 'x-elastic-client-meta': 'es=9.0.1,py=3.13.3,t=8.17.1,ur=2.4.0'}
retries = Retry(total=False, connect=None, read=None, redirect=0, status=None), redirect = True
assert_same_host = True, timeout = &lt;_TYPE_DEFAULT.token: -1&gt;, pool_timeout = None
release_conn = True, chunked = False, body_pos = None, preload_content = True
decode_content = True, response_kw = {}
parsed_url = Url(scheme=None, auth=None, host=None, port=None, path='/autocura-testes-2025.05.21/_doc', query=None, fragment=None)
destination_scheme = None, conn = None, release_this_conn = True, http_tunnel_required = False
err = None, clean_exit = False

    def urlopen(  # type: ignore[override]
        self,
        method: str,
        url: str,
        body: _TYPE_BODY | None = None,
        headers: typing.Mapping[str, str] | None = None,
        retries: Retry | bool | int | None = None,
        redirect: bool = True,
        assert_same_host: bool = True,
        timeout: _TYPE_TIMEOUT = _DEFAULT_TIMEOUT,
        pool_timeout: int | None = None,
        release_conn: bool | None = None,
        chunked: bool = False,
        body_pos: _TYPE_BODY_POSITION | None = None,
        preload_content: bool = True,
        decode_content: bool = True,
        **response_kw: typing.Any,
    ) -&gt; BaseHTTPResponse:
        """
        Get a connection from the pool and perform an HTTP request. This is the
        lowest level call for making a request, so you'll need to specify all
        the raw details.
    
        .. note::
    
           More commonly, it's appropriate to use a convenience method
           such as :meth:`request`.
    
        .. note::
    
           `release_conn` will only behave as expected if
           `preload_content=False` because we want to make
           `preload_content=False` the default behaviour someday soon without
           breaking backwards compatibility.
    
        :param method:
            HTTP request method (such as GET, POST, PUT, etc.)
    
        :param url:
            The URL to perform the request on.
    
        :param body:
            Data to send in the request body, either :class:`str`, :class:`bytes`,
            an iterable of :class:`str`/:class:`bytes`, or a file-like object.
    
        :param headers:
            Dictionary of custom headers to send, such as User-Agent,
            If-None-Match, etc. If None, pool headers are used. If provided,
            these headers completely replace any pool-specific headers.
    
        :param retries:
            Configure the number of retries to allow before raising a
            :class:`~urllib3.exceptions.MaxRetryError` exception.
    
            If ``None`` (default) will retry 3 times, see ``Retry.DEFAULT``. Pass a
            :class:`~urllib3.util.retry.Retry` object for fine-grained control
            over different types of retries.
            Pass an integer number to retry connection errors that many times,
            but no other types of errors. Pass zero to never retry.
    
            If ``False``, then retries are disabled and any exception is raised
            immediately. Also, instead of raising a MaxRetryError on redirects,
            the redirect response will be returned.
    
        :type retries: :class:`~urllib3.util.retry.Retry`, False, or an int.
    
        :param redirect:
            If True, automatically handle redirects (status codes 301, 302,
            303, 307, 308). Each redirect counts as a retry. Disabling retries
            will disable redirect, too.
    
        :param assert_same_host:
            If ``True``, will make sure that the host of the pool requests is
            consistent else will raise HostChangedError. When ``False``, you can
            use the pool on an HTTP proxy and request foreign hosts.
    
        :param timeout:
            If specified, overrides the default timeout for this one
            request. It may be a float (in seconds) or an instance of
            :class:`urllib3.util.Timeout`.
    
        :param pool_timeout:
            If set and the pool is set to block=True, then this method will
            block for ``pool_timeout`` seconds and raise EmptyPoolError if no
            connection is available within the time period.
    
        :param bool preload_content:
            If True, the response's body will be preloaded into memory.
    
        :param bool decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.
    
        :param release_conn:
            If False, then the urlopen call will not release the connection
            back into the pool once a response is received (but will release if
            you read the entire contents of the response such as when
            `preload_content=True`). This is useful if you're not preloading
            the response's content immediately. You will need to call
            ``r.release_conn()`` on the response ``r`` to return the connection
            back into the pool. If None, it takes the value of ``preload_content``
            which defaults to ``True``.
    
        :param bool chunked:
            If True, urllib3 will send the body using chunked transfer
            encoding. Otherwise, urllib3 will send the body using the standard
            content-length form. Defaults to False.
    
        :param int body_pos:
            Position to seek to in file-like body in the event of a retry or
            redirect. Typically this won't need to be set because urllib3 will
            auto-populate the value when needed.
        """
        parsed_url = parse_url(url)
        destination_scheme = parsed_url.scheme
    
        if headers is None:
            headers = self.headers
    
        if not isinstance(retries, Retry):
            retries = Retry.from_int(retries, redirect=redirect, default=self.retries)
    
        if release_conn is None:
            release_conn = preload_content
    
        # Check host
        if assert_same_host and not self.is_same_host(url):
            raise HostChangedError(self, url, retries)
    
        # Ensure that the URL we're connecting to is properly encoded
        if url.startswith("/"):
            url = to_str(_encode_target(url))
        else:
            url = to_str(parsed_url.url)
    
        conn = None
    
        # Track whether `conn` needs to be released before
        # returning/raising/recursing. Update this variable if necessary, and
        # leave `release_conn` constant throughout the function. That way, if
        # the function recurses, the original value of `release_conn` will be
        # passed down into the recursive call, and its value will be respected.
        #
        # See issue #651 [1] for details.
        #
        # [1] &lt;https://github.com/urllib3/urllib3/issues/651&gt;
        release_this_conn = release_conn
    
        http_tunnel_required = connection_requires_http_tunnel(
            self.proxy, self.proxy_config, destination_scheme
        )
    
        # Merge the proxy headers. Only done when not using HTTP CONNECT. We
        # have to copy the headers dict so we can safely change it without those
        # changes being reflected in anyone else's copy.
        if not http_tunnel_required:
            headers = headers.copy()  # type: ignore[attr-defined]
            headers.update(self.proxy_headers)  # type: ignore[union-attr]
    
        # Must keep the exception bound to a separate variable or else Python 3
        # complains about UnboundLocalError.
        err = None
    
        # Keep track of whether we cleanly exited the except block. This
        # ensures we do proper cleanup in finally.
        clean_exit = False
    
        # Rewind body position, if needed. Record current position
        # for future rewinds in the event of a redirect/retry.
        body_pos = set_file_position(body, body_pos)
    
        try:
            # Request a connection from the queue.
            timeout_obj = self._get_timeout(timeout)
            conn = self._get_conn(timeout=pool_timeout)
    
            conn.timeout = timeout_obj.connect_timeout  # type: ignore[assignment]
    
            # Is this a closed/new connection that requires CONNECT tunnelling?
            if self.proxy is not None and http_tunnel_required and conn.is_closed:
                try:
                    self._prepare_proxy(conn)
                except (BaseSSLError, OSError, SocketTimeout) as e:
                    self._raise_timeout(
                        err=e, url=self.proxy.url, timeout_value=conn.timeout
                    )
                    raise
    
            # If we're going to release the connection in ``finally:``, then
            # the response doesn't need to know about the connection. Otherwise
            # it will also try to release it and we'll have a double-release
            # mess.
            response_conn = conn if not release_conn else None
    
            # Make the request on the HTTPConnection object
&gt;           response = self._make_request(
                conn,
                method,
                url,
                timeout=timeout_obj,
                body=body,
                headers=headers,
                chunked=chunked,
                retries=retries,
                response_conn=response_conn,
                preload_content=preload_content,
                decode_content=decode_content,
                **response_kw,
            )

C:\Python313\Lib\site-packages\urllib3\connectionpool.py:787: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;urllib3.connectionpool.HTTPConnectionPool object at 0x000001E7F154DA90&gt;
conn = &lt;urllib3.connection.HTTPConnection object at 0x000001E7F1B91F30&gt;, method = 'POST'
url = '/autocura-testes-2025.05.21/_doc'
body = b'{"timestamp":"2025-05-21T16:07:18.546552","nome":"test_processamento_concorrente","sucesso":true,"duracao":1.8835067749023438e-05}'
headers = {'user-agent': 'elasticsearch-py/9.0.1 (Python/3.13.3; elastic-transport/8.17.1)', 'connection': 'keep-alive', 'accept...pplication/vnd.elasticsearch+json; compatible-with=9', 'x-elastic-client-meta': 'es=9.0.1,py=3.13.3,t=8.17.1,ur=2.4.0'}
retries = Retry(total=False, connect=None, read=None, redirect=0, status=None)
timeout = Timeout(connect=&lt;_TYPE_DEFAULT.token: -1&gt;, read=&lt;_TYPE_DEFAULT.token: -1&gt;, total=10.0)
chunked = False, response_conn = None, preload_content = True, decode_content = True
enforce_content_length = True

    def _make_request(
        self,
        conn: BaseHTTPConnection,
        method: str,
        url: str,
        body: _TYPE_BODY | None = None,
        headers: typing.Mapping[str, str] | None = None,
        retries: Retry | None = None,
        timeout: _TYPE_TIMEOUT = _DEFAULT_TIMEOUT,
        chunked: bool = False,
        response_conn: BaseHTTPConnection | None = None,
        preload_content: bool = True,
        decode_content: bool = True,
        enforce_content_length: bool = True,
    ) -&gt; BaseHTTPResponse:
        """
        Perform a request on a given urllib connection object taken from our
        pool.
    
        :param conn:
            a connection from one of our connection pools
    
        :param method:
            HTTP request method (such as GET, POST, PUT, etc.)
    
        :param url:
            The URL to perform the request on.
    
        :param body:
            Data to send in the request body, either :class:`str`, :class:`bytes`,
            an iterable of :class:`str`/:class:`bytes`, or a file-like object.
    
        :param headers:
            Dictionary of custom headers to send, such as User-Agent,
            If-None-Match, etc. If None, pool headers are used. If provided,
            these headers completely replace any pool-specific headers.
    
        :param retries:
            Configure the number of retries to allow before raising a
            :class:`~urllib3.exceptions.MaxRetryError` exception.
    
            Pass ``None`` to retry until you receive a response. Pass a
            :class:`~urllib3.util.retry.Retry` object for fine-grained control
            over different types of retries.
            Pass an integer number to retry connection errors that many times,
            but no other types of errors. Pass zero to never retry.
    
            If ``False``, then retries are disabled and any exception is raised
            immediately. Also, instead of raising a MaxRetryError on redirects,
            the redirect response will be returned.
    
        :type retries: :class:`~urllib3.util.retry.Retry`, False, or an int.
    
        :param timeout:
            If specified, overrides the default timeout for this one
            request. It may be a float (in seconds) or an instance of
            :class:`urllib3.util.Timeout`.
    
        :param chunked:
            If True, urllib3 will send the body using chunked transfer
            encoding. Otherwise, urllib3 will send the body using the standard
            content-length form. Defaults to False.
    
        :param response_conn:
            Set this to ``None`` if you will handle releasing the connection or
            set the connection to have the response release it.
    
        :param preload_content:
          If True, the response's body will be preloaded during construction.
    
        :param decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.
    
        :param enforce_content_length:
            Enforce content length checking. Body returned by server must match
            value of Content-Length header, if present. Otherwise, raise error.
        """
        self.num_requests += 1
    
        timeout_obj = self._get_timeout(timeout)
        timeout_obj.start_connect()
        conn.timeout = Timeout.resolve_default_timeout(timeout_obj.connect_timeout)
    
        try:
            # Trigger any extra validation we need to do.
            try:
                self._validate_conn(conn)
            except (SocketTimeout, BaseSSLError) as e:
                self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)
                raise
    
        # _validate_conn() starts the connection to an HTTPS proxy
        # so we need to wrap errors with 'ProxyError' here too.
        except (
            OSError,
            NewConnectionError,
            TimeoutError,
            BaseSSLError,
            CertificateError,
            SSLError,
        ) as e:
            new_e: Exception = e
            if isinstance(e, (BaseSSLError, CertificateError)):
                new_e = SSLError(e)
            # If the connection didn't successfully connect to it's proxy
            # then there
            if isinstance(
                new_e, (OSError, NewConnectionError, TimeoutError, SSLError)
            ) and (conn and conn.proxy and not conn.has_connected_to_proxy):
                new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)
            raise new_e
    
        # conn.request() calls http.client.*.request, not the method in
        # urllib3.request. It also calls makefile (recv) on the socket.
        try:
&gt;           conn.request(
                method,
                url,
                body=body,
                headers=headers,
                chunked=chunked,
                preload_content=preload_content,
                decode_content=decode_content,
                enforce_content_length=enforce_content_length,
            )

C:\Python313\Lib\site-packages\urllib3\connectionpool.py:493: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;urllib3.connection.HTTPConnection object at 0x000001E7F1B91F30&gt;, method = 'POST'
url = '/autocura-testes-2025.05.21/_doc'
body = b'{"timestamp":"2025-05-21T16:07:18.546552","nome":"test_processamento_concorrente","sucesso":true,"duracao":1.8835067749023438e-05}'
headers = {'user-agent': 'elasticsearch-py/9.0.1 (Python/3.13.3; elastic-transport/8.17.1)', 'connection': 'keep-alive', 'accept...pplication/vnd.elasticsearch+json; compatible-with=9', 'x-elastic-client-meta': 'es=9.0.1,py=3.13.3,t=8.17.1,ur=2.4.0'}

    def request(  # type: ignore[override]
        self,
        method: str,
        url: str,
        body: _TYPE_BODY | None = None,
        headers: typing.Mapping[str, str] | None = None,
        *,
        chunked: bool = False,
        preload_content: bool = True,
        decode_content: bool = True,
        enforce_content_length: bool = True,
    ) -&gt; None:
        # Update the inner socket's timeout value to send the request.
        # This only triggers if the connection is re-used.
        if self.sock is not None:
            self.sock.settimeout(self.timeout)
    
        # Store these values to be fed into the HTTPResponse
        # object later. TODO: Remove this in favor of a real
        # HTTP lifecycle mechanism.
    
        # We have to store these before we call .request()
        # because sometimes we can still salvage a response
        # off the wire even if we aren't able to completely
        # send the request body.
        self._response_options = _ResponseOptions(
            request_method=method,
            request_url=url,
            preload_content=preload_content,
            decode_content=decode_content,
            enforce_content_length=enforce_content_length,
        )
    
        if headers is None:
            headers = {}
        header_keys = frozenset(to_str(k.lower()) for k in headers)
        skip_accept_encoding = "accept-encoding" in header_keys
        skip_host = "host" in header_keys
        self.putrequest(
            method, url, skip_accept_encoding=skip_accept_encoding, skip_host=skip_host
        )
    
        # Transform the body into an iterable of sendall()-able chunks
        # and detect if an explicit Content-Length is doable.
        chunks_and_cl = body_to_chunks(body, method=method, blocksize=self.blocksize)
        chunks = chunks_and_cl.chunks
        content_length = chunks_and_cl.content_length
    
        # When chunked is explicit set to 'True' we respect that.
        if chunked:
            if "transfer-encoding" not in header_keys:
                self.putheader("Transfer-Encoding", "chunked")
        else:
            # Detect whether a framing mechanism is already in use. If so
            # we respect that value, otherwise we pick chunked vs content-length
            # depending on the type of 'body'.
            if "content-length" in header_keys:
                chunked = False
            elif "transfer-encoding" in header_keys:
                chunked = True
    
            # Otherwise we go off the recommendation of 'body_to_chunks()'.
            else:
                chunked = False
                if content_length is None:
                    if chunks is not None:
                        chunked = True
                        self.putheader("Transfer-Encoding", "chunked")
                else:
                    self.putheader("Content-Length", str(content_length))
    
        # Now that framing headers are out of the way we send all the other headers.
        if "user-agent" not in header_keys:
            self.putheader("User-Agent", _get_default_user_agent())
        for header, value in headers.items():
            self.putheader(header, value)
&gt;       self.endheaders()

C:\Python313\Lib\site-packages\urllib3\connection.py:445: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;urllib3.connection.HTTPConnection object at 0x000001E7F1B91F30&gt;, message_body = None

    def endheaders(self, message_body=None, *, encode_chunked=False):
        """Indicate that the last header line has been sent to the server.
    
        This method sends the request to the server.  The optional message_body
        argument can be used to pass a message body associated with the
        request.
        """
        if self.__state == _CS_REQ_STARTED:
            self.__state = _CS_REQ_SENT
        else:
            raise CannotSendHeader()
&gt;       self._send_output(message_body, encode_chunked=encode_chunked)

C:\Python313\Lib\http\client.py:1333: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;urllib3.connection.HTTPConnection object at 0x000001E7F1B91F30&gt;, message_body = None
encode_chunked = False

    def _send_output(self, message_body=None, encode_chunked=False):
        """Send the currently buffered request and clear the buffer.
    
        Appends an extra \\r\\n to the buffer.
        A message_body may be specified, to be appended to the request.
        """
        self._buffer.extend((b"", b""))
        msg = b"\r\n".join(self._buffer)
        del self._buffer[:]
&gt;       self.send(msg)

C:\Python313\Lib\http\client.py:1093: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;urllib3.connection.HTTPConnection object at 0x000001E7F1B91F30&gt;
data = b'POST /autocura-testes-2025.05.21/_doc HTTP/1.1\r\nHost: localhost:9200\r\nAccept-Encoding: identity\r\nContent-Lengt...ation/vnd.elasticsearch+json; compatible-with=9\r\nx-elastic-client-meta: es=9.0.1,py=3.13.3,t=8.17.1,ur=2.4.0\r\n\r\n'

    def send(self, data):
        """Send `data' to the server.
        ``data`` can be a string object, a bytes object, an array object, a
        file-like object that supports a .read() method, or an iterable object.
        """
    
        if self.sock is None:
            if self.auto_open:
&gt;               self.connect()

C:\Python313\Lib\http\client.py:1037: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;urllib3.connection.HTTPConnection object at 0x000001E7F1B91F30&gt;

    def connect(self) -&gt; None:
&gt;       self.sock = self._new_conn()

C:\Python313\Lib\site-packages\urllib3\connection.py:276: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;urllib3.connection.HTTPConnection object at 0x000001E7F1B91F30&gt;

    def _new_conn(self) -&gt; socket.socket:
        """Establish a socket connection and set nodelay settings on it.
    
        :return: New socket connection.
        """
        try:
            sock = connection.create_connection(
                (self._dns_host, self.port),
                self.timeout,
                source_address=self.source_address,
                socket_options=self.socket_options,
            )
        except socket.gaierror as e:
            raise NameResolutionError(self.host, self, e) from e
        except SocketTimeout as e:
            raise ConnectTimeoutError(
                self,
                f"Connection to {self.host} timed out. (connect timeout={self.timeout})",
            ) from e
    
        except OSError as e:
&gt;           raise NewConnectionError(
                self, f"Failed to establish a new connection: {e}"
            ) from e
E           urllib3.exceptions.NewConnectionError: &lt;urllib3.connection.HTTPConnection object at 0x000001E7F1B91F30&gt;: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente

C:\Python313\Lib\site-packages\urllib3\connection.py:213: NewConnectionError

The above exception was the direct cause of the following exception:

self = &lt;tests.integration.test_sistema_completo.TestSistemaCompleto object at 0x000001E7F138B360&gt;
config_teste = {'ambiente': 'teste', 'api_url': 'http://localhost:8000', 'db_url': 'postgresql://test:test@localhost:5432/test_db', 'redis_url': 'redis://localhost:6379/0', ...}
mock_api = &lt;MagicMock name='Session' id='2095697780016'&gt;
mock_db = &lt;MagicMock name='create_engine' id='2095701933568'&gt;
mock_redis = &lt;MagicMock name='Redis' id='2095704948816'&gt;
monitoramento = &lt;src.orquestrador.monitoramento.MonitoramentoTestes object at 0x000001E7F1451810&gt;

    def test_processamento_concorrente(
        self,
        config_teste: Dict[str, Any],
        mock_api: Any,
        mock_db: Any,
        mock_redis: Any,
        monitoramento: Any
    ) -&gt; None:
        """
        Testa o processamento concorrente de múltiplos dados.
    
        Este teste verifica:
        1. Processamento paralelo
        2. Consistência dos dados
        3. Performance
    
        Args:
            config_teste: Configurações de teste
            mock_api: Mock da API
            mock_db: Mock do banco de dados
            mock_redis: Mock do Redis
            monitoramento: Instância do monitoramento
        """
        # Arrange
        dados_entrada = [
            {"id": f"test_{i}", "dados": [i], "timestamp": datetime.now().isoformat()}
            for i in range(10)
        ]
    
        # Act
        inicio = time.time()
        resultados = self._processar_dados_concorrente(dados_entrada, config_teste)
        duracao = time.time() - inicio
    
        # Assert
        assert len(resultados) == len(dados_entrada)
        assert all(r["status"] == "success" for r in resultados)
    
        # Verifica métricas
&gt;       monitoramento.registrar_execucao_teste(
            "test_processamento_concorrente",
            True,
            duracao
        )

tests\integration\test_sistema_completo.py:142: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;src.orquestrador.monitoramento.MonitoramentoTestes object at 0x000001E7F1451810&gt;
nome = 'test_processamento_concorrente', sucesso = True, duracao = 1.8835067749023438e-05

    def registrar_execucao_teste(self, nome: str, sucesso: bool, duracao: float) -&gt; None:
        """
        Registra a execução de um teste.
    
        Args:
            nome: Nome do teste
            sucesso: Se o teste passou
            duracao: Duração em segundos
        """
        # Prometheus
        if hasattr(self, "metricas"):
            self.metricas["testes_executados"].inc()
            self.metricas["duracao_testes"].observe(duracao)
    
            if not sucesso:
                self.metricas["testes_falhas"].inc()
    
        # Elasticsearch
        if hasattr(self, "es"):
            doc = {
                "timestamp": datetime.now().isoformat(),
                "nome": nome,
                "sucesso": sucesso,
                "duracao": duracao
            }
    
&gt;           self.es.index(
                index=f"{self.config['elk']['elasticsearch']['indice_prefixo']}{datetime.now().strftime('%Y.%m.%d')}",
                document=doc
            )

src\orquestrador\monitoramento.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (&lt;Elasticsearch(['http://localhost:9200'])&gt;,)
kwargs = {'document': {'duracao': 1.8835067749023438e-05, 'nome': 'test_processamento_concorrente', 'sucesso': True, 'timestamp': '2025-05-21T16:07:18.546552'}, 'index': 'autocura-testes-2025.05.21'}
maybe_transport_options = set()

    @wraps(api)
    def wrapped(*args: Any, **kwargs: Any) -&gt; Any:
        # Let's give a nicer error message when users pass positional arguments.
        if len(args) &gt;= 2:
            raise TypeError(
                "Positional arguments can't be used with Elasticsearch API methods. "
                "Instead only use keyword arguments."
            )
    
        # We merge 'params' first as transport options can be specified using params.
        if "params" in kwargs and (
            not ignore_deprecated_options
            or "params" not in ignore_deprecated_options
        ):
            params = kwargs.pop("params")
            if params:
                if not hasattr(params, "items"):
                    raise ValueError(
                        "Couldn't merge 'params' with other parameters as it wasn't a mapping. "
                        "Instead of using 'params' use individual API parameters"
                    )
                warnings.warn(
                    "The 'params' parameter is deprecated and will be removed "
                    "in a future version. Instead use individual parameters.",
                    category=DeprecationWarning,
                    stacklevel=warn_stacklevel(),
                )
                _merge_kwargs_no_duplicates(kwargs, params)
    
        maybe_transport_options = _TRANSPORT_OPTIONS.intersection(kwargs)
        if maybe_transport_options:
            transport_options = {}
            for option in maybe_transport_options:
                if (
                    ignore_deprecated_options
                    and option in ignore_deprecated_options
                ):
                    continue
                try:
                    option_rename = option
                    if option == "ignore":
                        option_rename = "ignore_status"
                    transport_options[option_rename] = kwargs.pop(option)
                except KeyError:
                    pass
            if transport_options:
                warnings.warn(
                    "Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.",
                    category=DeprecationWarning,
                    stacklevel=warn_stacklevel(),
                )
                client = args[0]
    
                # Namespaced clients need to unwrapped.
                namespaced_client: Optional[Type["NamespacedClient"]] = None
                if hasattr(client, "_client"):
                    namespaced_client = type(client)
                    client = client._client
    
                client = client.options(**transport_options)
    
                # Re-wrap the client if we unwrapped due to being namespaced.
                if namespaced_client is not None:
                    client = namespaced_client(client)
                args = (client,) + args[1:]
    
        if "body" in kwargs and (
            not ignore_deprecated_options or "body" not in ignore_deprecated_options
        ):
            body: Optional[_TYPE_BODY] = kwargs.pop("body")
            mixed_body_and_params = False
            if body is not None:
                if body_name:
                    if body_name in kwargs:
                        raise TypeError(
                            f"Can't use '{body_name}' and 'body' parameters together because '{body_name}' "
                            "is an alias for 'body'. Instead you should only use the "
                            f"'{body_name}' parameter. See https://github.com/elastic/elasticsearch-py/"
                            "issues/1698 for more information"
                        )
                    kwargs[body_name] = body
                elif body_fields is not None:
                    mixed_body_and_params = _merge_body_fields_no_duplicates(
                        body, kwargs, body_fields
                    )
                    kwargs["body"] = body
    
                if parameter_aliases and not isinstance(body, (str, bytes)):
                    for alias, rename_to in parameter_aliases.items():
                        if rename_to in body:
                            body[alias] = body.pop(rename_to)
                            # If body and params are mixed, the alias may come from a param,
                            # in which case the warning below will not make sense.
                            if not mixed_body_and_params:
                                warnings.warn(
                                    f"Using '{rename_to}' alias in 'body' is deprecated and will be removed "
                                    f"in a future version of elasticsearch-py. Use '{alias}' directly instead. "
                                    "See https://github.com/elastic/elasticsearch-py/issues/1698 for more information",
                                    category=DeprecationWarning,
                                    stacklevel=2,
                                )
    
        if parameter_aliases:
            for alias, rename_to in parameter_aliases.items():
                try:
                    kwargs[rename_to] = kwargs.pop(alias)
                except KeyError:
                    pass
    
&gt;       return api(*args, **kwargs)

C:\Python313\Lib\site-packages\elasticsearch\_sync\client\utils.py:415: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Elasticsearch(['http://localhost:9200'])&gt;

    @_rewrite_parameters(
        body_name="document",
    )
    def index(
        self,
        *,
        index: str,
        document: t.Optional[t.Mapping[str, t.Any]] = None,
        body: t.Optional[t.Mapping[str, t.Any]] = None,
        id: t.Optional[str] = None,
        error_trace: t.Optional[bool] = None,
        filter_path: t.Optional[t.Union[str, t.Sequence[str]]] = None,
        human: t.Optional[bool] = None,
        if_primary_term: t.Optional[int] = None,
        if_seq_no: t.Optional[int] = None,
        include_source_on_error: t.Optional[bool] = None,
        op_type: t.Optional[t.Union[str, t.Literal["create", "index"]]] = None,
        pipeline: t.Optional[str] = None,
        pretty: t.Optional[bool] = None,
        refresh: t.Optional[
            t.Union[bool, str, t.Literal["false", "true", "wait_for"]]
        ] = None,
        require_alias: t.Optional[bool] = None,
        routing: t.Optional[str] = None,
        timeout: t.Optional[t.Union[str, t.Literal[-1], t.Literal[0]]] = None,
        version: t.Optional[int] = None,
        version_type: t.Optional[
            t.Union[str, t.Literal["external", "external_gte", "force", "internal"]]
        ] = None,
        wait_for_active_shards: t.Optional[
            t.Union[int, t.Union[str, t.Literal["all", "index-setting"]]]
        ] = None,
    ) -&gt; ObjectApiResponse[t.Any]:
        """
        .. raw:: html
    
          &lt;p&gt;Create or update a document in an index.&lt;/p&gt;
          &lt;p&gt;Add a JSON document to the specified data stream or index and make it searchable.
          If the target is an index and the document already exists, the request updates the document and increments its version.&lt;/p&gt;
          &lt;p&gt;NOTE: You cannot use this API to send update requests for existing documents in a data stream.&lt;/p&gt;
          &lt;p&gt;If the Elasticsearch security features are enabled, you must have the following index privileges for the target data stream, index, or index alias:&lt;/p&gt;
          &lt;ul&gt;
          &lt;li&gt;To add or overwrite a document using the &lt;code&gt;PUT /&amp;lt;target&amp;gt;/_doc/&amp;lt;_id&amp;gt;&lt;/code&gt; request format, you must have the &lt;code&gt;create&lt;/code&gt;, &lt;code&gt;index&lt;/code&gt;, or &lt;code&gt;write&lt;/code&gt; index privilege.&lt;/li&gt;
          &lt;li&gt;To add a document using the &lt;code&gt;POST /&amp;lt;target&amp;gt;/_doc/&lt;/code&gt; request format, you must have the &lt;code&gt;create_doc&lt;/code&gt;, &lt;code&gt;create&lt;/code&gt;, &lt;code&gt;index&lt;/code&gt;, or &lt;code&gt;write&lt;/code&gt; index privilege.&lt;/li&gt;
          &lt;li&gt;To automatically create a data stream or index with this API request, you must have the &lt;code&gt;auto_configure&lt;/code&gt;, &lt;code&gt;create_index&lt;/code&gt;, or &lt;code&gt;manage&lt;/code&gt; index privilege.&lt;/li&gt;
          &lt;/ul&gt;
          &lt;p&gt;Automatic data stream creation requires a matching index template with data stream enabled.&lt;/p&gt;
          &lt;p&gt;NOTE: Replica shards might not all be started when an indexing operation returns successfully.
          By default, only the primary is required. Set &lt;code&gt;wait_for_active_shards&lt;/code&gt; to change this default behavior.&lt;/p&gt;
          &lt;p&gt;&lt;strong&gt;Automatically create data streams and indices&lt;/strong&gt;&lt;/p&gt;
          &lt;p&gt;If the request's target doesn't exist and matches an index template with a &lt;code&gt;data_stream&lt;/code&gt; definition, the index operation automatically creates the data stream.&lt;/p&gt;
          &lt;p&gt;If the target doesn't exist and doesn't match a data stream template, the operation automatically creates the index and applies any matching index templates.&lt;/p&gt;
          &lt;p&gt;NOTE: Elasticsearch includes several built-in index templates. To avoid naming collisions with these templates, refer to index pattern documentation.&lt;/p&gt;
          &lt;p&gt;If no mapping exists, the index operation creates a dynamic mapping.
          By default, new fields and objects are automatically added to the mapping if needed.&lt;/p&gt;
          &lt;p&gt;Automatic index creation is controlled by the &lt;code&gt;action.auto_create_index&lt;/code&gt; setting.
          If it is &lt;code&gt;true&lt;/code&gt;, any index can be created automatically.
          You can modify this setting to explicitly allow or block automatic creation of indices that match specified patterns or set it to &lt;code&gt;false&lt;/code&gt; to turn off automatic index creation entirely.
          Specify a comma-separated list of patterns you want to allow or prefix each pattern with &lt;code&gt;+&lt;/code&gt; or &lt;code&gt;-&lt;/code&gt; to indicate whether it should be allowed or blocked.
          When a list is specified, the default behaviour is to disallow.&lt;/p&gt;
          &lt;p&gt;NOTE: The &lt;code&gt;action.auto_create_index&lt;/code&gt; setting affects the automatic creation of indices only.
          It does not affect the creation of data streams.&lt;/p&gt;
          &lt;p&gt;&lt;strong&gt;Optimistic concurrency control&lt;/strong&gt;&lt;/p&gt;
          &lt;p&gt;Index operations can be made conditional and only be performed if the last modification to the document was assigned the sequence number and primary term specified by the &lt;code&gt;if_seq_no&lt;/code&gt; and &lt;code&gt;if_primary_term&lt;/code&gt; parameters.
          If a mismatch is detected, the operation will result in a &lt;code&gt;VersionConflictException&lt;/code&gt; and a status code of &lt;code&gt;409&lt;/code&gt;.&lt;/p&gt;
          &lt;p&gt;&lt;strong&gt;Routing&lt;/strong&gt;&lt;/p&gt;
          &lt;p&gt;By default, shard placement — or routing — is controlled by using a hash of the document's ID value.
          For more explicit control, the value fed into the hash function used by the router can be directly specified on a per-operation basis using the &lt;code&gt;routing&lt;/code&gt; parameter.&lt;/p&gt;
          &lt;p&gt;When setting up explicit mapping, you can also use the &lt;code&gt;_routing&lt;/code&gt; field to direct the index operation to extract the routing value from the document itself.
          This does come at the (very minimal) cost of an additional document parsing pass.
          If the &lt;code&gt;_routing&lt;/code&gt; mapping is defined and set to be required, the index operation will fail if no routing value is provided or extracted.&lt;/p&gt;
          &lt;p&gt;NOTE: Data streams do not support custom routing unless they were created with the &lt;code&gt;allow_custom_routing&lt;/code&gt; setting enabled in the template.&lt;/p&gt;
          &lt;p&gt;&lt;strong&gt;Distributed&lt;/strong&gt;&lt;/p&gt;
          &lt;p&gt;The index operation is directed to the primary shard based on its route and performed on the actual node containing this shard.
          After the primary shard completes the operation, if needed, the update is distributed to applicable replicas.&lt;/p&gt;
          &lt;p&gt;&lt;strong&gt;Active shards&lt;/strong&gt;&lt;/p&gt;
          &lt;p&gt;To improve the resiliency of writes to the system, indexing operations can be configured to wait for a certain number of active shard copies before proceeding with the operation.
          If the requisite number of active shard copies are not available, then the write operation must wait and retry, until either the requisite shard copies have started or a timeout occurs.
          By default, write operations only wait for the primary shards to be active before proceeding (that is to say &lt;code&gt;wait_for_active_shards&lt;/code&gt; is &lt;code&gt;1&lt;/code&gt;).
          This default can be overridden in the index settings dynamically by setting &lt;code&gt;index.write.wait_for_active_shards&lt;/code&gt;.
          To alter this behavior per operation, use the &lt;code&gt;wait_for_active_shards request&lt;/code&gt; parameter.&lt;/p&gt;
          &lt;p&gt;Valid values are all or any positive integer up to the total number of configured copies per shard in the index (which is &lt;code&gt;number_of_replicas&lt;/code&gt;+1).
          Specifying a negative value or a number greater than the number of shard copies will throw an error.&lt;/p&gt;
          &lt;p&gt;For example, suppose you have a cluster of three nodes, A, B, and C and you create an index index with the number of replicas set to 3 (resulting in 4 shard copies, one more copy than there are nodes).
          If you attempt an indexing operation, by default the operation will only ensure the primary copy of each shard is available before proceeding.
          This means that even if B and C went down and A hosted the primary shard copies, the indexing operation would still proceed with only one copy of the data.
          If &lt;code&gt;wait_for_active_shards&lt;/code&gt; is set on the request to &lt;code&gt;3&lt;/code&gt; (and all three nodes are up), the indexing operation will require 3 active shard copies before proceeding.
          This requirement should be met because there are 3 active nodes in the cluster, each one holding a copy of the shard.
          However, if you set &lt;code&gt;wait_for_active_shards&lt;/code&gt; to &lt;code&gt;all&lt;/code&gt; (or to &lt;code&gt;4&lt;/code&gt;, which is the same in this situation), the indexing operation will not proceed as you do not have all 4 copies of each shard active in the index.
          The operation will timeout unless a new node is brought up in the cluster to host the fourth copy of the shard.&lt;/p&gt;
          &lt;p&gt;It is important to note that this setting greatly reduces the chances of the write operation not writing to the requisite number of shard copies, but it does not completely eliminate the possibility, because this check occurs before the write operation starts.
          After the write operation is underway, it is still possible for replication to fail on any number of shard copies but still succeed on the primary.
          The &lt;code&gt;_shards&lt;/code&gt; section of the API response reveals the number of shard copies on which replication succeeded and failed.&lt;/p&gt;
          &lt;p&gt;&lt;strong&gt;No operation (noop) updates&lt;/strong&gt;&lt;/p&gt;
          &lt;p&gt;When updating a document by using this API, a new version of the document is always created even if the document hasn't changed.
          If this isn't acceptable use the &lt;code&gt;_update&lt;/code&gt; API with &lt;code&gt;detect_noop&lt;/code&gt; set to &lt;code&gt;true&lt;/code&gt;.
          The &lt;code&gt;detect_noop&lt;/code&gt; option isn't available on this API because it doesn’t fetch the old source and isn't able to compare it against the new source.&lt;/p&gt;
          &lt;p&gt;There isn't a definitive rule for when noop updates aren't acceptable.
          It's a combination of lots of factors like how frequently your data source sends updates that are actually noops and how many queries per second Elasticsearch runs on the shard receiving the updates.&lt;/p&gt;
          &lt;p&gt;&lt;strong&gt;Versioning&lt;/strong&gt;&lt;/p&gt;
          &lt;p&gt;Each indexed document is given a version number.
          By default, internal versioning is used that starts at 1 and increments with each update, deletes included.
          Optionally, the version number can be set to an external value (for example, if maintained in a database).
          To enable this functionality, &lt;code&gt;version_type&lt;/code&gt; should be set to &lt;code&gt;external&lt;/code&gt;.
          The value provided must be a numeric, long value greater than or equal to 0, and less than around &lt;code&gt;9.2e+18&lt;/code&gt;.&lt;/p&gt;
          &lt;p&gt;NOTE: Versioning is completely real time, and is not affected by the near real time aspects of search operations.
          If no version is provided, the operation runs without any version checks.&lt;/p&gt;
          &lt;p&gt;When using the external version type, the system checks to see if the version number passed to the index request is greater than the version of the currently stored document.
          If true, the document will be indexed and the new version number used.
          If the value provided is less than or equal to the stored document's version number, a version conflict will occur and the index operation will fail. For example:&lt;/p&gt;
          &lt;pre&gt;&lt;code&gt;PUT my-index-000001/_doc/1?version=2&amp;amp;version_type=external
          {
            &amp;quot;user&amp;quot;: {
              &amp;quot;id&amp;quot;: &amp;quot;elkbee&amp;quot;
            }
          }
    
          In this example, the operation will succeed since the supplied version of 2 is higher than the current document version of 1.
          If the document was already updated and its version was set to 2 or higher, the indexing command will fail and result in a conflict (409 HTTP status code).
    
          A nice side effect is that there is no need to maintain strict ordering of async indexing operations run as a result of changes to a source database, as long as version numbers from the source database are used.
          Even the simple case of updating the Elasticsearch index using data from a database is simplified if external versioning is used, as only the latest version will be used if the index operations arrive out of order.
          &lt;/code&gt;&lt;/pre&gt;
    
    
        `&lt;https://www.elastic.co/docs/api/doc/elasticsearch/v9/operation/operation-create&gt;`_
    
        :param index: The name of the data stream or index to target. If the target doesn't
            exist and matches the name or wildcard (`*`) pattern of an index template
            with a `data_stream` definition, this request creates the data stream. If
            the target doesn't exist and doesn't match a data stream template, this request
            creates the index. You can check for existing targets with the resolve index
            API.
        :param document:
        :param id: A unique identifier for the document. To automatically generate a
            document ID, use the `POST /&lt;target&gt;/_doc/` request format and omit this
            parameter.
        :param if_primary_term: Only perform the operation if the document has this primary
            term.
        :param if_seq_no: Only perform the operation if the document has this sequence
            number.
        :param include_source_on_error: True or false if to include the document source
            in the error message in case of parsing errors.
        :param op_type: Set to `create` to only index the document if it does not already
            exist (put if absent). If a document with the specified `_id` already exists,
            the indexing operation will fail. The behavior is the same as using the `&lt;index&gt;/_create`
            endpoint. If a document ID is specified, this paramater defaults to `index`.
            Otherwise, it defaults to `create`. If the request targets a data stream,
            an `op_type` of `create` is required.
        :param pipeline: The ID of the pipeline to use to preprocess incoming documents.
            If the index has a default ingest pipeline specified, then setting the value
            to `_none` disables the default ingest pipeline for this request. If a final
            pipeline is configured it will always run, regardless of the value of this
            parameter.
        :param refresh: If `true`, Elasticsearch refreshes the affected shards to make
            this operation visible to search. If `wait_for`, it waits for a refresh to
            make this operation visible to search. If `false`, it does nothing with refreshes.
        :param require_alias: If `true`, the destination must be an index alias.
        :param routing: A custom value that is used to route operations to a specific
            shard.
        :param timeout: The period the request waits for the following operations: automatic
            index creation, dynamic mapping updates, waiting for active shards. This
            parameter is useful for situations where the primary shard assigned to perform
            the operation might not be available when the operation runs. Some reasons
            for this might be that the primary shard is currently recovering from a gateway
            or undergoing relocation. By default, the operation will wait on the primary
            shard to become available for at least 1 minute before failing and responding
            with an error. The actual wait time could be longer, particularly when multiple
            waits occur.
        :param version: An explicit version number for concurrency control. It must be
            a non-negative long number.
        :param version_type: The version type.
        :param wait_for_active_shards: The number of shard copies that must be active
            before proceeding with the operation. You can set it to `all` or any positive
            integer up to the total number of shards in the index (`number_of_replicas+1`).
            The default value of `1` means it waits for each primary shard to be active.
        """
        if index in SKIP_IN_PATH:
            raise ValueError("Empty value passed for parameter 'index'")
        if document is None and body is None:
            raise ValueError(
                "Empty value passed for parameters 'document' and 'body', one of them should be set."
            )
        elif document is not None and body is not None:
            raise ValueError("Cannot set both 'document' and 'body'")
        __path_parts: t.Dict[str, str]
        if index not in SKIP_IN_PATH and id not in SKIP_IN_PATH:
            __path_parts = {"index": _quote(index), "id": _quote(id)}
            __path = f'/{__path_parts["index"]}/_doc/{__path_parts["id"]}'
            __method = "PUT"
        elif index not in SKIP_IN_PATH:
            __path_parts = {"index": _quote(index)}
            __path = f'/{__path_parts["index"]}/_doc'
            __method = "POST"
        else:
            raise ValueError("Couldn't find a path for the given parameters")
        __query: t.Dict[str, t.Any] = {}
        if error_trace is not None:
            __query["error_trace"] = error_trace
        if filter_path is not None:
            __query["filter_path"] = filter_path
        if human is not None:
            __query["human"] = human
        if if_primary_term is not None:
            __query["if_primary_term"] = if_primary_term
        if if_seq_no is not None:
            __query["if_seq_no"] = if_seq_no
        if include_source_on_error is not None:
            __query["include_source_on_error"] = include_source_on_error
        if op_type is not None:
            __query["op_type"] = op_type
        if pipeline is not None:
            __query["pipeline"] = pipeline
        if pretty is not None:
            __query["pretty"] = pretty
        if refresh is not None:
            __query["refresh"] = refresh
        if require_alias is not None:
            __query["require_alias"] = require_alias
        if routing is not None:
            __query["routing"] = routing
        if timeout is not None:
            __query["timeout"] = timeout
        if version is not None:
            __query["version"] = version
        if version_type is not None:
            __query["version_type"] = version_type
        if wait_for_active_shards is not None:
            __query["wait_for_active_shards"] = wait_for_active_shards
        __body = document if document is not None else body
        __headers = {"accept": "application/json", "content-type": "application/json"}
&gt;       return self.perform_request(  # type: ignore[return-value]
            __method,
            __path,
            params=__query,
            headers=__headers,
            body=__body,
            endpoint_id="index",
            path_parts=__path_parts,
        )

C:\Python313\Lib\site-packages\elasticsearch\_sync\client\__init__.py:2951: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Elasticsearch(['http://localhost:9200'])&gt;, method = 'POST'
path = '/autocura-testes-2025.05.21/_doc'

    def perform_request(
        self,
        method: str,
        path: str,
        *,
        params: Optional[Mapping[str, Any]] = None,
        headers: Optional[Mapping[str, str]] = None,
        body: Optional[Any] = None,
        endpoint_id: Optional[str] = None,
        path_parts: Optional[Mapping[str, Any]] = None,
    ) -&gt; ApiResponse[Any]:
        with self._otel.span(
            method,
            endpoint_id=endpoint_id,
            path_parts=path_parts or {},
        ) as otel_span:
&gt;           response = self._perform_request(
                method,
                path,
                params=params,
                headers=headers,
                body=body,
                otel_span=otel_span,
            )

C:\Python313\Lib\site-packages\elasticsearch\_sync\client\_base.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Elasticsearch(['http://localhost:9200'])&gt;, method = 'POST'
path = '/autocura-testes-2025.05.21/_doc'

    def _perform_request(
        self,
        method: str,
        path: str,
        *,
        params: Optional[Mapping[str, Any]] = None,
        headers: Optional[Mapping[str, str]] = None,
        body: Optional[Any] = None,
        otel_span: OpenTelemetrySpan,
    ) -&gt; ApiResponse[Any]:
        if headers:
            request_headers = self._headers.copy()
            request_headers.update(headers)
        else:
            request_headers = self._headers
    
        def mimetype_header_to_compat(header: str) -&gt; None:
            # Converts all parts of a Accept/Content-Type headers
            # from application/X -&gt; application/vnd.elasticsearch+X
            mimetype = request_headers.get(header, None)
            if mimetype:
                request_headers[header] = _COMPAT_MIMETYPE_RE.sub(
                    _COMPAT_MIMETYPE_SUB, mimetype
                )
    
        mimetype_header_to_compat("Accept")
        mimetype_header_to_compat("Content-Type")
    
        if params:
            target = f"{path}?{_quote_query(params)}"
        else:
            target = path
    
&gt;       meta, resp_body = self.transport.perform_request(
            method,
            target,
            headers=request_headers,
            body=body,
            request_timeout=self._request_timeout,
            max_retries=self._max_retries,
            retry_on_status=self._retry_on_status,
            retry_on_timeout=self._retry_on_timeout,
            client_meta=self._client_meta,
            otel_span=otel_span,
        )

C:\Python313\Lib\site-packages\elasticsearch\_sync\client\_base.py:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;elastic_transport.Transport object at 0x000001E7F154D310&gt;, method = 'POST'
target = '/autocura-testes-2025.05.21/_doc'

    def perform_request(  # type: ignore[return]
        self,
        method: str,
        target: str,
        *,
        body: Optional[Any] = None,
        headers: Union[Mapping[str, Any], DefaultType] = DEFAULT,
        max_retries: Union[int, DefaultType] = DEFAULT,
        retry_on_status: Union[Collection[int], DefaultType] = DEFAULT,
        retry_on_timeout: Union[bool, DefaultType] = DEFAULT,
        request_timeout: Union[Optional[float], DefaultType] = DEFAULT,
        client_meta: Union[Tuple[Tuple[str, str], ...], DefaultType] = DEFAULT,
        otel_span: Union[OpenTelemetrySpan, DefaultType] = DEFAULT,
    ) -&gt; TransportApiResponse:
        """
        Perform the actual request. Retrieve a node from the node
        pool, pass all the information to it's perform_request method and
        return the data.
    
        If an exception was raised, mark the node as failed and retry (up
        to ``max_retries`` times).
    
        If the operation was successful and the node used was previously
        marked as dead, mark it as live, resetting it's failure count.
    
        :arg method: HTTP method to use
        :arg target: HTTP request target
        :arg body: body of the request, will be serialized using serializer and
            passed to the node
        :arg headers: Additional headers to send with the request.
        :arg max_retries: Maximum number of retries before giving up on a request.
            Set to ``0`` to disable retries.
        :arg retry_on_status: Collection of HTTP status codes to retry.
        :arg retry_on_timeout: Set to true to retry after timeout errors.
        :arg request_timeout: Amount of time to wait for a response to fail with a timeout error.
        :arg client_meta: Extra client metadata key-value pairs to send in the client meta header.
        :arg otel_span: OpenTelemetry span used to add metadata to the span.
    
        :returns: Tuple of the :class:`elastic_transport.ApiResponseMeta` with the deserialized response.
        """
        if headers is DEFAULT:
            request_headers = HttpHeaders()
        else:
            request_headers = HttpHeaders(headers)
        max_retries = resolve_default(max_retries, self.max_retries)
        retry_on_timeout = resolve_default(retry_on_timeout, self.retry_on_timeout)
        retry_on_status = resolve_default(retry_on_status, self.retry_on_status)
        otel_span = resolve_default(otel_span, OpenTelemetrySpan(None))
    
        if self.meta_header:
            request_headers["x-elastic-client-meta"] = ",".join(
                f"{k}={v}"
                for k, v in self._transport_client_meta
                + resolve_default(client_meta, ())
            )
    
        # Serialize the request body to bytes based on the given mimetype.
        request_body: Optional[bytes]
        if body is not None:
            if "content-type" not in request_headers:
                raise ValueError(
                    "Must provide a 'Content-Type' header to requests with bodies"
                )
            request_body = self.serializers.dumps(
                body, mimetype=request_headers["content-type"]
            )
            otel_span.set_db_statement(request_body)
        else:
            request_body = None
    
        # Errors are stored from (oldest-&gt;newest)
        errors: List[Exception] = []
    
        for attempt in range(max_retries + 1):
            # If we sniff before requests are made we want to do so before
            # 'node_pool.get()' is called so our sniffed nodes show up in the pool.
            if self._sniff_before_requests:
                self.sniff(False)
    
            retry = False
            node_failure = False
            last_response: Optional[TransportApiResponse] = None
            node = self.node_pool.get()
            start_time = time.time()
            try:
                otel_span.set_node_metadata(node.host, node.port, node.base_url, target)
&gt;               resp = node.perform_request(
                    method,
                    target,
                    body=request_body,
                    headers=request_headers,
                    request_timeout=request_timeout,
                )

C:\Python313\Lib\site-packages\elastic_transport\_transport.py:342: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;Urllib3HttpNode(http://localhost:9200)&gt;, method = 'POST'
target = '/autocura-testes-2025.05.21/_doc'
body = b'{"timestamp":"2025-05-21T16:07:18.546552","nome":"test_processamento_concorrente","sucesso":true,"duracao":1.8835067749023438e-05}'
headers = {'accept': 'application/vnd.elasticsearch+json; compatible-with=9', 'content-type': 'application/vnd.elasticsearch+json; compatible-with=9', 'x-elastic-client-meta': 'es=9.0.1,py=3.13.3,t=8.17.1,ur=2.4.0'}
request_timeout = &lt;DEFAULT&gt;

    def perform_request(
        self,
        method: str,
        target: str,
        body: Optional[bytes] = None,
        headers: Optional[HttpHeaders] = None,
        request_timeout: Union[DefaultType, Optional[float]] = DEFAULT,
    ) -&gt; NodeApiResponse:
        if self.path_prefix:
            target = f"{self.path_prefix}{target}"
    
        start = time.time()
        try:
            kw = {}
            if request_timeout is not DEFAULT:
                kw["timeout"] = request_timeout
    
            request_headers = self._headers.copy()
            if headers:
                request_headers.update(headers)
    
            body_to_send: Optional[bytes]
            if body:
                if self._http_compress:
                    body_to_send = gzip.compress(body)
                    request_headers["content-encoding"] = "gzip"
                else:
                    body_to_send = body
            else:
                body_to_send = None
    
            response = self.pool.urlopen(
                method,
                target,
                body=body_to_send,
                retries=Retry(False),
                headers=request_headers,
                **kw,  # type: ignore[arg-type]
            )
            response_headers = HttpHeaders(response.headers)
            data = response.data
            duration = time.time() - start
    
        except RERAISE_EXCEPTIONS:
            raise
        except Exception as e:
            err: Exception
            if isinstance(e, NewConnectionError):
                err = ConnectionError(str(e), errors=(e,))
            elif isinstance(e, (ConnectTimeoutError, ReadTimeoutError)):
                err = ConnectionTimeout(
                    "Connection timed out during request", errors=(e,)
                )
            elif isinstance(e, (ssl.SSLError, urllib3.exceptions.SSLError)):
                err = TlsError(str(e), errors=(e,))
            elif isinstance(e, BUILTIN_EXCEPTIONS):
                raise
            else:
                err = ConnectionError(str(e), errors=(e,))
            self._log_request(
                method=method,
                target=target,
                headers=request_headers,
                body=body,
                exception=err,
            )
&gt;           raise err from e
E           elastic_transport.ConnectionError: Connection error caused by: ConnectionError(Connection error caused by: NewConnectionError(&lt;urllib3.connection.HTTPConnection object at 0x000001E7F1BFC2F0&gt;: Failed to establish a new connection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente))

C:\Python313\Lib\site-packages\elastic_transport\_node\_http_urllib3.py:202: ConnectionError</failure></testcase></testsuite></testsuites>