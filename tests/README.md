# Testes do Sistema de Autocura

Este diretÃ³rio contÃ©m os testes automatizados do sistema de autocura. Os testes sÃ£o organizados em diferentes categorias e seguem as melhores prÃ¡ticas de desenvolvimento.

## ğŸ“ Estrutura de DiretÃ³rios

```
tests/
â”œâ”€â”€ unit/              # Testes unitÃ¡rios
â”‚   â”œâ”€â”€ core/         # Testes dos mÃ³dulos core
â”‚   â”œâ”€â”€ autocura/     # Testes dos mÃ³dulos de autocura
â”‚   â”œâ”€â”€ etica/        # Testes dos mÃ³dulos Ã©ticos
â”‚   â””â”€â”€ ...
â”œâ”€â”€ integration/       # Testes de integraÃ§Ã£o
â”‚   â”œâ”€â”€ api/          # Testes de API
â”‚   â”œâ”€â”€ database/     # Testes de banco de dados
â”‚   â””â”€â”€ ...
â””â”€â”€ e2e/              # Testes end-to-end
    â””â”€â”€ scenarios/    # CenÃ¡rios de teste
```

## ğŸš€ Executando os Testes

### PrÃ©-requisitos

1. Instale as dependÃªncias de teste:
```bash
pip install -r requirements-test.txt
```

2. Configure o ambiente:
```bash
cp .env.example .env.test
# Edite .env.test com as configuraÃ§Ãµes de teste
```

### ExecuÃ§Ã£o

1. **Executar todos os testes**:
```bash
python scripts/run_tests.py
```

2. **Executar testes especÃ­ficos**:
```bash
# Testes unitÃ¡rios
python scripts/run_tests.py tests/unit/

# Testes de integraÃ§Ã£o
python scripts/run_tests.py tests/integration/

# Testes end-to-end
python scripts/run_tests.py tests/e2e/
```

3. **Executar com marcadores**:
```bash
# Testes assÃ­ncronos
python scripts/run_tests.py -m async

# Testes de banco de dados
python scripts/run_tests.py -m database

# Testes de API
python scripts/run_tests.py -m api
```

## ğŸ“Š RelatÃ³rios

Os relatÃ³rios sÃ£o gerados automaticamente apÃ³s a execuÃ§Ã£o dos testes:

- **RelatÃ³rio HTML**: `test-results/report.html`
- **RelatÃ³rio JUnit**: `test-results/junit.xml`
- **Cobertura de CÃ³digo**: `htmlcov/index.html`
- **RelatÃ³rio Markdown**: `test-results/test_report.md`

## ğŸ¯ Cobertura de CÃ³digo

A cobertura mÃ­nima de cÃ³digo Ã© de 80%. Para verificar a cobertura:

```bash
python scripts/run_tests.py --cov-report=term-missing
```

## ğŸ” Marcadores de Teste

- `@pytest.mark.unit`: Testes unitÃ¡rios
- `@pytest.mark.integration`: Testes de integraÃ§Ã£o
- `@pytest.mark.e2e`: Testes end-to-end
- `@pytest.mark.slow`: Testes que demoram mais
- `@pytest.mark.async`: Testes assÃ­ncronos
- `@pytest.mark.memory`: Testes de memÃ³ria
- `@pytest.mark.database`: Testes de banco de dados
- `@pytest.mark.api`: Testes de API
- `@pytest.mark.security`: Testes de seguranÃ§a
- `@pytest.mark.performance`: Testes de performance

## ğŸ“ Escrevendo Testes

### Estrutura BÃ¡sica

```python
import pytest
from unittest.mock import Mock, patch

@pytest.fixture
def fixture_exemplo():
    """Fixture de exemplo."""
    return {
        "dado": "valor"
    }

@pytest.mark.asyncio
async def test_exemplo(fixture_exemplo):
    """Teste de exemplo."""
    # Arrange
    valor = fixture_exemplo["dado"]
    
    # Act
    resultado = await funcao_teste(valor)
    
    # Assert
    assert resultado == valor_esperado
```

### Boas PrÃ¡ticas

1. **Nomenclatura**:
   - Arquivos: `test_*.py`
   - FunÃ§Ãµes: `test_*`
   - Classes: `Test*`

2. **OrganizaÃ§Ã£o**:
   - Um arquivo por mÃ³dulo testado
   - Testes relacionados agrupados
   - Fixtures compartilhadas no `conftest.py`

3. **Asserts**:
   - Use asserts especÃ­ficos
   - Verifique exceÃ§Ãµes quando apropriado
   - Teste casos de erro

4. **Mocks**:
   - Use mocks para dependÃªncias externas
   - Evite mocks desnecessÃ¡rios
   - Documente o comportamento esperado

## ğŸ”§ ConfiguraÃ§Ã£o

O arquivo `pytest.ini` contÃ©m as configuraÃ§Ãµes principais:

```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

addopts = 
    --verbose
    --tb=long
    --cov=src
    --cov-report=html
    --cov-report=term-missing
    --cov-report=xml
    --cov-fail-under=80
```

## ğŸ› ï¸ UtilitÃ¡rios

### Fixtures Comuns

- `config_teste`: ConfiguraÃ§Ãµes de teste
- `mock_api`: Mock da API
- `mock_db`: Mock do banco de dados
- `mock_redis`: Mock do Redis
- `dados_teste`: Dados de exemplo
- `monitoramento`: InstÃ¢ncia do monitoramento
- `temp_dir`: DiretÃ³rio temporÃ¡rio
- `log_captura`: Captura de logs
- `mock_env`: VariÃ¡veis de ambiente
- `mock_time`: Mock de tempo

### Helpers

- `assert_dict_contains`: Verifica se dicionÃ¡rio contÃ©m chaves
- `assert_list_contains`: Verifica se lista contÃ©m itens
- `assert_async_raises`: Verifica exceÃ§Ãµes assÃ­ncronas
- `assert_log_contains`: Verifica mensagens de log

## ğŸ” SeguranÃ§a

1. **Dados SensÃ­veis**:
   - Use variÃ¡veis de ambiente
   - NÃ£o comite credenciais
   - Use mocks para serviÃ§os externos

2. **Ambiente de Teste**:
   - Isolado do ambiente de produÃ§Ã£o
   - Dados de teste anÃ´nimos
   - Limpeza apÃ³s execuÃ§Ã£o

## ğŸ“ˆ Monitoramento

Os testes sÃ£o monitorados atravÃ©s de:

- MÃ©tricas de cobertura
- Tempo de execuÃ§Ã£o
- Taxa de falhas
- TendÃªncias de performance

## ğŸ¤ Contribuindo

1. Siga o padrÃ£o de cÃ³digo
2. Adicione testes para novas funcionalidades
3. Mantenha a cobertura acima de 80%
4. Documente mudanÃ§as significativas
5. Atualize este README quando necessÃ¡rio 